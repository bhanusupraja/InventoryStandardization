# InventoryStandardization

Step 1: Create a Gemini api key from the this website
Step 2: Add the generated key to the .env file
GEMINI_API_KEY = ""
Step 3: open terminal go agents path and create a virtual venv and activate it 
Step 4: run the command below
pip install -r requirements.txt
Step 5: run the command below to start the agent
python inputProcessing.py file then you could see the outputs in the data folder

## Detailed version
Hereâ€™s a **detailed,  **InventoryStandardization** project â€” it explains setup, configuration, agent roles, and expected outputs clearly ğŸ‘‡

---

# ğŸ§  InventoryStandardization

A modular AI-powered pipeline designed to **standardize, clean, and generate structured product data (SKUs, confidence scores, mappings)** from raw multi-category inventory files such as Jewelry, Textiles, Electronics, and more.

---

## ğŸ“¦ Overview

The `InventoryStandardization` project uses a series of AI and rule-based **agents** to automate inventory data transformation and SKU generation.

The goal is to:

* Process messy raw inventory data files (with 60+ columns)
* Extract and normalize only the relevant fields
* Generate standardized **SKUs**
* Compute **confidence scores**
* Route low-confidence records to **HITL (Human-in-the-Loop)** review

---

## âš™ï¸ Architecture Overview

The complete pipeline includes the following agents:

| Agent                                   | Purpose                                                                              | Type              |
| --------------------------------------- | ------------------------------------------------------------------------------------ | ----------------- |
| ğŸ§¹ **InputProcessingAgent**             | Cleans & extracts necessary fields from raw CSVs (e.g. Brand, Category, Description) | Non-AI / Hybrid   |
| ğŸ§  **ConfidenceSKUAgent**               | Generates standardized SKUs & assigns confidence scores (row + column level)         | AI-assisted       |
| ğŸ” **MappingEngineAgent**               | Classifies data into known internal schemas & identifies missing/ambiguous fields    | ML Classifier     |
| ğŸ‘¤ **HITL Review Agent**                | Routes low-confidence mappings (< threshold) to a review interface                   | Human-in-the-Loop |
| ğŸ“Š **AuditReportingAgent** *(optional)* | Aggregates logs, generates audit reports for QA                                      | Non-AI            |

---

## ğŸª„ Features

âœ… Automatic SKU generation from cleaned input
âœ… Confidence scoring for both row-level & attribute-level data
âœ… Multi-domain support (Jewelry, Textiles, Electronics, etc.)
âœ… AI-assisted data enrichment and normalization
âœ… Human-in-the-loop fallback for uncertain predictions
âœ… Exports final standardized datasets to `.csv` and `.parquet`

---

## ğŸ§° Tech Stack

* **Python 3.10+**
* **Pandas** for data handling
* **FastAPI (optional)** for service orchestration
* **Google Gemini API** for AI-powered text interpretation
* **dotenv** for key management
* **scikit-learn** (planned) for classification and mapping engine

---

## ğŸ§© Setup Instructions

### 1ï¸âƒ£ Generate a Gemini API Key

Visit [https://aistudio.google.com/app/apikey](https://aistudio.google.com/app/apikey)
Copy your key and keep it secure.

---

### 2ï¸âƒ£ Add API Key to `.env`

Create a `.env` file in the project root and paste:

```bash
GEMINI_API_KEY = "your_gemini_api_key_here"
```

---

### 3ï¸âƒ£ Create and Activate a Virtual Environment

In the project root:

```bash
cd agents
python -m venv venv
# Activate venv
# On Windows:
venv\Scripts\activate
# On Mac/Linux:
source venv/bin/activate
```

---

### 4ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

---

### 5ï¸âƒ£ Run the Input Processing Agent

```bash
python inputProcessing.py
```

This step:

* Reads raw data from `data/raw/`
* Cleans and extracts essential fields
* Saves the intermediate output to `data/processed/`

---

### 6ï¸âƒ£ Run the Confidence + SKU Agent

```bash
python confidence_sku_agent.py
```

This step:

* Loads the processed file from `data/processed/`
* Generates standardized **SKUs**
* Calculates **confidence scores**
* Flags low-confidence rows for review
* Saves final results to:

```
data/output/standardized_inventory.csv
data/output/standardized_inventory.parquet
```

---

## ğŸ“ˆ Sample Output

| SKU               | Brand     | Category | Description        | Confidence_Score | Review_Flag |
| ----------------- | --------- | -------- | ------------------ | ---------------- | ----------- |
| JW-BR-GLD-NEC-001 | Bluestone | Jewelry  | Gold Necklace 18K  | 0.94             | No          |
| TX-FB-COT-SHR-022 | FabIndia  | Textile  | Cotton Shirt Menâ€™s | 0.87             | Yes         |

---

## ğŸ§  Recommended Development Order

1ï¸âƒ£ `inputProcessing.py` â†’ Data extraction and normalization
2ï¸âƒ£ `confidence_sku_agent.py` â†’ SKU generation + confidence scoring
3ï¸âƒ£ `mapping_engine.py` â†’ ML-based attribute classifier
4ï¸âƒ£ `hitl_agent.py` â†’ HITL flag routing + review dashboard
5ï¸âƒ£ `audit_reporting.py` *(optional)* â†’ Logging and QA reports

---

## ğŸ§¾ Example Folder Structure

InventoryStandardization/
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ inputProcessing.py
â”‚   â”œâ”€â”€ confidence_sku_agent.py
â”‚   â”œâ”€â”€ mapping_engine.py
â”‚   â”œâ”€â”€ hitl_agent.py
â”‚   â””â”€â”€ audit_reporting.py
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ output/
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
